version: "3.9"

services:
  # TTS (Kokoro) - interno (no expuesto al público)
  kokoro:
    build:
      context: ./inference/kokoro
    networks:
      - agent_network
    restart: unless-stopped

  # LiveKit - expone señalización + RTC (incluye UDP para WebRTC)
  livekit:
    image: livekit/livekit-server:latest
    command: --dev --bind "0.0.0.0"
    ports:
      - "7880:7880"        # HTTP/WS signaling
      - "7881:7881"        # RTC over TCP
      - "7882:7882/udp"    # RTC over UDP (muy importante)
    networks:
      - agent_network
    restart: unless-stopped

  # STT (Whisper/VoxBox) - interno (no expuesto al público)
  whisper:
    build:
      context: ./inference/whisper
    volumes:
      - whisper-data:/data
    environment:
      - VOXBOX_HF_REPO_ID=${VOXBOX_HF_REPO_ID:-Systran/faster-whisper-small}
      - VOXBOX_DEVICE=${VOXBOX_DEVICE:-cpu}
      - DATA_DIR=/data
    networks:
      - agent_network
    restart: unless-stopped

  # LLM (llama.cpp server) - interno (no expuesto al público)
  # Ajustado para VPS de 8GB: ctx más bajo para reducir RAM.
  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    command:
      - --host
      - 0.0.0.0
      - --port
      - "11434"
      - --hf-repo
      - "${LLAMA_HF_REPO:-unsloth/Qwen3-4B-Instruct-2507-GGUF}"
      - --alias
      - "${LLAMA_MODEL_ALIAS:-qwen3-4b}"
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-4096}"
    volumes:
      - ./inference/llama/models:/models
    environment:
      - XDG_CACHE_HOME=/models
      - HF_HOME=/models
    networks:
      - agent_network
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:11434/v1/models > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  # Agente LiveKit
  livekit_agent:
    build:
      context: ./livekit_agent
    env_file:
      - .env
    environment:
      - LIVEKIT_URL=${LIVEKIT_URL:-ws://livekit:7880}
      - LIVEKIT_HOST=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET:-secret}
      - LIVEKIT_AGENT_PORT=${LIVEKIT_AGENT_PORT:-7880}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-no-key-needed}
      - GROQ_API_KEY=${GROQ_API_KEY:-no-key-needed}
    depends_on:
      livekit:
        condition: service_started
      kokoro:
        condition: service_started
      whisper:
        condition: service_started
      llama_cpp:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8081/ > /dev/null"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - agent_network
    restart: unless-stopped

  # Frontend (Next.js)
  frontend:
    build:
      context: ./frontend
    ports:
      - "3000:3000"
    environment:
      # IMPORTANTE: en VPS NO uses localhost.
      # Default puesto a tu IP del VPS; también puedes sobreescribirlo con .env:
      NEXT_PUBLIC_LIVEKIT_URL=ws://72.62.82.20:7880
      - NEXT_PUBLIC_LIVEKIT_URL=${NEXT_PUBLIC_LIVEKIT_URL:-ws://72.62.82.20:7880}
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET:-secret}
    depends_on:
      livekit:
        condition: service_started
    networks:
      - agent_network
    restart: unless-stopped

volumes:
  whisper-data:

networks:
  agent_network:
    driver: bridge
