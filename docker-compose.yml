version: "3.9"

services:
  kokoro:
    build:
      context: ./inference/kokoro
    networks:
      - agent_network
    restart: unless-stopped

  livekit:
    image: livekit/livekit-server:latest
    command: --dev --bind "0.0.0.0"
    ports:
      - "7880:7880"        # signaling (HTTP/WS)
      - "7881:7881"        # RTC over TCP
      - "7882:7882/udp"    # RTC over UDP (necesario para WebRTC)
    networks:
      - agent_network
    restart: unless-stopped

  whisper:
    build:
      context: ./inference/whisper
    volumes:
      - whisper-data:/data
    environment:
      VOXBOX_HF_REPO_ID: ${VOXBOX_HF_REPO_ID:-Systran/faster-whisper-small}
      VOXBOX_DEVICE: ${VOXBOX_DEVICE:-cpu}
      DATA_DIR: /data
    networks:
      - agent_network
    restart: unless-stopped

  llama_cpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    command:
      - --host
      - 0.0.0.0
      - --port
      - "11434"
      - --hf-repo
      - "${LLAMA_HF_REPO:-unsloth/Qwen3-4B-Instruct-2507-GGUF}"
      - --alias
      - "${LLAMA_MODEL_ALIAS:-qwen3-4b}"
      - --ctx-size
      - "${LLAMA_CTX_SIZE:-4096}"
    volumes:
      - ./inference/llama/models:/models
    environment:
      XDG_CACHE_HOME: /models
      HF_HOME: /models
    networks:
      - agent_network
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:11434/v1/models > /dev/null"]
      interval: 10s
      timeout: 5s
      retries: 30
    restart: unless-stopped

  livekit_agent:
    build:
      context: ./livekit_agent
    env_file:
      - .env
    environment:
      LIVEKIT_URL: ${LIVEKIT_URL:-ws://livekit:7880}
      LIVEKIT_HOST: ws://livekit:7880
      LIVEKIT_API_KEY: ${LIVEKIT_API_KEY:-devkey}
      LIVEKIT_API_SECRET: ${LIVEKIT_API_SECRET:-secret}
      LIVEKIT_AGENT_PORT: ${LIVEKIT_AGENT_PORT:-7880}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-no-key-needed}
      GROQ_API_KEY: ${GROQ_API_KEY:-no-key-needed}
    depends_on:
      livekit:
        condition: service_started
      kokoro:
        condition: service_started
      whisper:
        condition: service_started
      llama_cpp:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8081/ > /dev/null"]
      interval: 5s
      timeout: 3s
      retries: 5
      start_period: 10s
    networks:
      - agent_network
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      args:
        # IMPORTANTÍSIMO: esto entra al build de Next.js (evita que se hornee localhost)
        NEXT_PUBLIC_LIVEKIT_URL: "ws://72.62.82.20:7880"
    ports:
      - "3000:3000"
    environment:
      # También lo dejamos en runtime (no hace daño), pero lo crítico es el build arg
      NEXT_PUBLIC_LIVEKIT_URL: "ws://72.62.82.20:7880"
      LIVEKIT_URL: "ws://livekit:7880"
      LIVEKIT_API_KEY: "devkey"
      LIVEKIT_API_SECRET: "secret"
    depends_on:
      livekit:
        condition: service_started
    networks:
      - agent_network
    restart: unless-stopped

volumes:
  whisper-data:

networks:
  agent_network:
    driver: bridge
